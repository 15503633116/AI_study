# 神经网络优化算法

本节将会具体介绍如何通过方向传播算法（backpropagation）和梯度下降算法（gradient decent）调整神经网络中参数的取值。

梯度下降算法主要用于**优化单个参数的取值**，而反向传播算法给出了一个高效的方式在**所有参数上**使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小。反向传播算法是训练神经网络的核 心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值。神经网络模型中参数的优化过程直接决定了模型的质豆 ，是使用神经网络时非常重要的一步。

假设用 Θ 表示神经网络中的参数，损失函数 J（Θ）表示在给定的参数取值下，训练数据集上损失函数的大小，那么整个优化过程中可以抽象为一个寻找一个参数Θ，使的 J（Θ）最小。

梯度下降算法是最常用的神经网络优化方法，梯度下降算法会更新参数Θ，不断沿着梯度的反方向让参数朝着总损失更小的方向更新。

![](../image/tf-1.1.png)

