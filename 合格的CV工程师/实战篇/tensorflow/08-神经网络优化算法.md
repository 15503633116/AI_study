[TOC]



## 神经网络优化算法

本节将会具体介绍如何通过方向传播算法（backpropagation）和梯度下降算法（gradient decent）调整神经网络中参数的取值。

梯度下降算法主要用于**优化单个参数的取值**，而反向传播算法给出了一个高效的方式在**所有参数上**使用梯度下降算法，从而使神经网络模型在训练数据上的损失函数尽可能小。反向传播算法是训练神经网络的核 心算法，它可以根据定义好的损失函数优化神经网络中参数的取值，从而使神经网络模型在训练数据集上的损失函数达到一个较小值。神经网络模型中参数的优化过程直接决定了模型的质豆 ，是使用神经网络时非常重要的一步。

假设用 Θ 表示神经网络中的参数，损失函数 J（Θ）表示在给定的参数取值下，训练数据集上损失函数的大小，那么整个优化过程中可以抽象为一个寻找一个参数Θ，使的 J（Θ）最小。

梯度下降算法是最常用的神经网络优化方法，梯度下降算法会更新参数Θ，不断沿着梯度的反方向让参数朝着总损失更小的方向更新。

![](../image/tf-1.1.png)

图4-11中 x 轴表示参数 Θ 的取值，y 轴表示损失函数 J（Θ）的值。假设小黑点是当前点，为了使损失函数 J（Θ）变小（朝着左边移动），此时梯度下降算法通过求偏导（曲线的话直接求导），还需要定义一个学习率 η 来定义每次参数更新的幅度。通过参数的梯度和学习率，参数更新公式为：

![](../image/tf-1.2.png)

对于梯度不理解的小伙伴可以参看：[梯度](https://baike.baidu.com/item/%E6%A2%AF%E5%BA%A6/13014729?fr=aladdin)

对于更新参数的理解，参数 Θ 更新实际上是要让损失函数变小，从图4-11上应该往左边移动，但是我们要以什么依据来决定移动的距离，科学家选择以梯度和学习率作为依据，但是实际上从上图可以直观的看出梯度是正的，如果参数更新是直接将 Θ 更新部分相加是不正确的，因为我们目标是希望让损失函数尽可能小，而梯度大于0此时损失函数会增大，所以在学习率和梯度前面加一个符号，才能达到我们的目标。

需要注意的是，**梯度下降算法并不能保证被优化的函数达到全局最优解**。如图4-12所示，在小黑点处，此时损失函数的偏导为0，于是参数就不会进一步更新。

最有当损失函数为凸函数时，梯度下降算法才能保证达到全局最优解。

![](../image/tf-1.3.png)

**梯度下降的另一个问题是计算时间太长**。因为要在全部训练数据上最小化损失，所以损失函数是在所有训练数据上的损失和 。这样在每一轮迭代中都需要计算在全部训练数据上的损失函数 。在海量训练数据下，要计算所有训练数据的损失函数是非常消耗时间的。

为了加速训练过程 , 可以使用随机梯度下降的算法 （ stochastic gradient descent ）。这个算法优化的不是在全部训练数据上的损失函数，而是在每一轮法代中,随机优化某一条训练数据上的损失函数 。 这样每一轮参数更新的速度就大大加快了。因为随机梯度下降算法每次优化的只是**某一条数据**上的损失函数，所以它的问题也非常明显：在某一条数据上损失函数更小并不代表在全部数据上损失函数更小，于是使用随机梯度下降优化得到的神经网络甚至可能无法达到局部最优。

为了综合梯度下降算法和随机梯度下降算法的优缺点。在实际应用中 一般采用这两个算法的折中——每次计算一小部分训练数据的损失函数。这一小部分数据被称之为一个batch。通过矩阵运算，每次在一个 batch 上优化神经网络的参数并不会比单个数据慢太多 。另一方面，每次使用 一个 batch 可以大大减小收敛所需要的法代次数，同时可以使收敛到的结果更加接近梯度下降的效果。



## 神经网络进一步优化

#### 学习率的设置

学习率决定了参数每次更新的幅度。学习率既不能过大，也不能过小。过大将可能导致不能收敛，过小会大大降低优化速度。

为了解决设定学习率的问题，TensorFlow提供了一种更加灵活的学习率设置方法——指数衰减法。

tf.train.exponential_decay函数实现了指数衰减学习率。

```
global_step = tf.Variable(0)
# 通过 exponential_decay 函数生成学习率
learning_rate = tf.train.exponential_decay(
				0.1,global_step,100,0.96,staircase=True)
# 使用指数衰减的学习惑。在 minimize 函数 中传入 global_step将自动更新
# global_step 参数,从而使得学习率也得到相应更新。
learning_step = tf.train.GradientDescentOptimizer(learning_rate)\
				.minimize(...my_loss ,)
```

上面这段代码中设定了初始学习率为 0.1 ,因为指定了 staircase=True ,所以每训练 100轮后学习率乘以 0.96 。 一般来说初始学习率、衰减系数和衰减速度都是根据经验设置的。而且损失函数下降的速度和法代结束之后总损失的大小没有必然的联系。也就是说并不能通过前几轮损失函数下降的速度来比较不同神经网络的效果。

#### 过拟合问题

