# 损失函数定义

神经网络模型的效果以及优化的目标是通过损失函数（loss function）来定义的 。

## 经典损失函数

分类问题和回归问题是监督学习的两大种类。

分类问题希望解决的是将不同的样本分到事先定义好的类别中。判断一个零件是否合格的问题就是一个二 分类问题。

交叉熵：刻画了两个概率分布之间的距离 , 它是分类问题中使用比较广的一种损失函数。

交叉熵是一个信息论中的概念,它原本是用来估算平均编码长度的。给定两个概率分布 p 和 q , 通过 q 来表示 p 的交叉煽为 :
$$
H(p,q)=-\sum_{x}p(x)log q(x)
$$
注意交叉熵：刻画的是两个概率分布之间的距离 , 然而神经网络的输出却不一定是一个概率分布。概率分布刻画了不同事件发生的概率。当事件总数有限的情况下 ,概率分布函数 p(X = x) 满足 :
$$
\forall x  \quad p(X=x)\epsilon[0,1]且\sum_{x}p(X=x)=1
$$
也就是说，任意事件发生的概率都在 0 和 1 之间，且总有某 一个事件发生 ( 概率的和为 1 )。

对于多分类问题，Softmax是一个非常常用的方法。Softmax 回归本身可以作为一个学习算法来优化分类结果，但在 TensorFlow中， Softmax回归的参数被去掉了，它只是一 层额外的处理层,将神经网络的输出变成一个概率分布。下图展示了加上了 Softmax 回归的神经网络结构图 。

![](../image/softmax.png)

假设原始的神经网络输出为 y1,y2...,那么经过 Softmax 回归处理之后的输出为 :
$$
softmax(y)_i=y_j'=\frac{e^{yj}}{\sum_{j=1}^{n}{e^{yj}}}
$$
从以上公式中可以看出，原始神经网络的输出被用作置信度来生成新的输出，而新的输出满足概率分布的所有要求。这个新的输出可以理解为经过神经网络的推导，一个样例为不同类别的概率分别是多大。这样就把神经网络的输出也变成了 一个概率分布，从而可以通过交叉熵来计算预测的概率分布和真实答案的概率分布之间的距离了。

从交叉熵的公式中可以看到交叉熵不是对称的 （H(p, q)不等于H(q,p )） ,它刻画的是通过概率分布 q 来表达概率分布 p 的困难程度。因为正确答案是希望得到的结果，所以当交叉熵作为神经网络的损失函数时， p 代表的是正确答案， q 代表的是预测值。交叉熵刻画的是两个概率分布的距离，也就是说交叉熵值越小，两个概率分布越接近。

## 自定义损失函数

在预测商品销量时，如果预测多了（预测值比真实销量大），商家损失的是生产商品的成本；而如果预测少了(预测值比真实销量小），损失的则是商品的利润。因为 一般商品的成本和商品的利润不会严格相 等，,所以使用均方误差损失函数就不能够很好地最大化销售利润 。 比如如果 一个商品的成本是1 元,但是利润是 10 元，,那么少预测一个就少挣 10 元；而多预测一个才少挣 1 元 。如果神经网络模型最小化的是均方误差，那么很有可能此模型就无法最大化预期的利润 。 为了最大化预期利润,需要将损失函数和利润直接联系起来 。注意损失函数定义的是损失，所以要将利润最大化,定义的损失函数应该刻画成本或者代价 。 以下公式给出了一个当预测多于真实值和预测少于真实值时有不同损失系数的损失函数:
$$
Loss(y,y')=\sum_{i=1}^{n}f(y_i,y_i'),\quad f(x,y)=\begin{cases} a(x-y)\quad x>y\\ b(y-x) \quad x \leq y \end{cases}
$$
TensorFlow中实现这个损失函数：

```
loss =tf.reduce_sum(tf.where(tf.greater(v1,v2),(v1-v2)*a,(v2-v1)*b))
```

以上代码用到了 tf.greater 和 tf.where 来实现选择操作 。 tf.greater 的输入是两个张量 ，此函数会比较这两个输入张量 中每 一个元素的大小，并返回比较结果 。

当 tf.greater 的输入张量维度不一样时, TensorFlow 会进行类似 NumPy 广播操作( broadcasting )的处理。tf.where函数有三个参数 。 第 一个为选择条件根据，当选择条件为 True 时，tf.where 函数会选择第二个参数中的值，否则使用第三个参数中的值。